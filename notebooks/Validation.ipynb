{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b8f9d0-7e50-4d73-9ef8-3af47ec1a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Generation ---\n",
      "True Dispersion (k): 3\n",
      "Mean of y: 5.7900, Var of y: 162.2509\n",
      "Ratio Var/Mean: 28.0226 (Expect > 1)\n",
      "\n",
      "--- Statsmodels (GLM) ---\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.5332      0.040     37.883      0.000       1.454       1.613\n",
      "x1            -0.5092      0.041    -12.496      0.000      -0.589      -0.429\n",
      "x2            -0.2434      0.041     -6.003      0.000      -0.323      -0.164\n",
      "x3            -0.4067      0.041     -9.935      0.000      -0.487      -0.326\n",
      "==============================================================================\n",
      "\n",
      "--- Custom NegativeBinomialRegressor ---\n",
      "Intercept: 1.5332\n",
      "Coefficients: [-0.50915684 -0.24337279 -0.40665888]\n",
      "\n",
      "--- Comparison (Absolute Difference) ---\n",
      "Intercept Diff: 0.000000\n",
      "Coef Max Diff : 0.000000\n",
      "SUCCESS: Models match closely!\n",
      "\n",
      "Mean Prediction Diff: 0.000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from glmext.glm import NegativeBinomialRegressor\n",
    "\n",
    "\n",
    "def generate_data(n_samples=1000, n_features=5, k=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for Negative Binomial regression.\n",
    "    k: Dispersion parameter (1/theta). Var = mu + k * mu^2\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = rng.randn(n_samples, n_features)\n",
    "    # True coefficients\n",
    "    coef = rng.randn(n_features) * 0.5\n",
    "    intercept = 1.5\n",
    "    # Linear predictor\n",
    "    linear_pred = np.dot(X, coef) + intercept\n",
    "    mu = np.exp(linear_pred)\n",
    "    # Generate y using Negative Binomial distribution\n",
    "    # scipy/numpy uses n (successes) and p (probability) parameterization\n",
    "    # mean = n * (1-p) / p\n",
    "    # var = mean / p\n",
    "    # Relation to k (alpha in statsmodels): n = 1/k, p = 1 / (1 + k*mu)\n",
    "    if k > 0:\n",
    "        n_nb = 1 / k\n",
    "        p_nb = 1 / (1 + k * mu)\n",
    "        y = rng.negative_binomial(n_nb, p_nb)\n",
    "    else:\n",
    "        y = rng.poisson(mu)\n",
    "    return X, y, coef, intercept\n",
    "\n",
    "\n",
    "def check_models():\n",
    "    # 1. Setup Data\n",
    "    n_samples = 2000\n",
    "    n_features = 3\n",
    "    k_true = 3 # Dispersion\n",
    "    X, y, true_coef, true_intercept = generate_data(n_samples, n_features, k=k_true)\n",
    "    print(f\"--- Data Generation ---\")\n",
    "    print(f\"True Dispersion (k): {k_true}\")\n",
    "    print(f\"Mean of y: {np.mean(y):.4f}, Var of y: {np.var(y):.4f}\")\n",
    "    print(f\"Ratio Var/Mean: {np.var(y)/np.mean(y):.4f} (Expect > 1)\\n\")\n",
    "\n",
    "    # 2. Statsmodels Implementation\n",
    "    # Statsmodels parameterizes NegativeBinomial with 'alpha' which corresponds to our 'k'\n",
    "    # We must assume 'alpha' is fixed to the known truth for a direct comparison of solvers,\n",
    "    # or we can let statsmodels estimate it.\n",
    "    # Here, we fit with the KNOWN k (alpha) to check if coefficients match.\n",
    "    # Add intercept for statsmodels\n",
    "    X_sm = sm.add_constant(X)\n",
    "    # statsmodels NegativeBinomial family\n",
    "    # loglike_method='nb2' corresponds to Var = mu + alpha * mu^2\n",
    "    sm_model = sm.GLM(\n",
    "        y,\n",
    "        X_sm,\n",
    "        family=sm.families.NegativeBinomial(alpha=k_true)\n",
    "    ).fit()\n",
    "\n",
    "    print(\"--- Statsmodels (GLM) ---\")\n",
    "    print(sm_model.summary().tables[1])\n",
    "    sm_coef = sm_model.params[1:]\n",
    "    sm_intercept = sm_model.params[0]\n",
    "    # 3. Custom Sklearn Implementation\n",
    "    # We set alpha=0 to remove regularization (matching statsmodels default)\n",
    "    # We fix k to the same value used in statsmodels\n",
    "    custom_model = NegativeBinomialRegressor(\n",
    "        k=k_true,\n",
    "        alpha=0.0, # No L2 regularization\n",
    "        fit_intercept=False,\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        tol=1e-6\n",
    "    )\n",
    "    custom_model.fit(\n",
    "        X_sm, y\n",
    "    )\n",
    "    print(\"\\n--- Custom NegativeBinomialRegressor ---\")\n",
    "    print(f\"Intercept: {custom_model.coef_[0]:.4f}\")\n",
    "    print(f\"Coefficients: {custom_model.coef_[1:]}\")\n",
    "    # 4. Comparison\n",
    "    print(\"\\n--- Comparison (Absolute Difference) ---\")\n",
    "    diff_intercept = abs(sm_intercept - custom_model.coef_[0])\n",
    "    diff_coef = np.abs(sm_coef - custom_model.coef_[1:])\n",
    "    print(f\"Intercept Diff: {diff_intercept:.6f}\")\n",
    "    print(f\"Coef Max Diff : {np.max(diff_coef):.6f}\")\n",
    "    if diff_intercept < 1e-3 and np.max(diff_coef) < 1e-3:\n",
    "        print(\"SUCCESS: Models match closely!\")\n",
    "    else:\n",
    "        print(\"Models diverge. Check regularization or tolerance.\")\n",
    "\n",
    "    # 5. Prediction Check\n",
    "    y_pred_sm = sm_model.predict(X_sm)\n",
    "    y_pred_custom = custom_model.predict(X_sm)\n",
    "    print(f\"\\nMean Prediction Diff: {np.mean(np.abs(y_pred_sm - y_pred_custom)):.6f}\")\n",
    "\n",
    "\n",
    "check_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cd45d2-a2d3-47f8-9be0-6fdcfc81e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Generation ---\n",
      "True Dispersion (k): 3\n",
      "Mean of y: 0.6195, Var of y: 0.2357\n",
      "Ratio Var/Mean: 0.3805 (Expect > 1)\n",
      "\n",
      "--- Statsmodels (GLM) ---\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5373      0.049     10.890      0.000       0.441       0.634\n",
      "x1            -0.3606      0.050     -7.263      0.000      -0.458      -0.263\n",
      "x2            -0.5790      0.052    -11.050      0.000      -0.682      -0.476\n",
      "x3            -0.3579      0.051     -7.046      0.000      -0.458      -0.258\n",
      "==============================================================================\n",
      "\n",
      "--- Custom BinomialRegressor ---\n",
      "Intercept: 0.5373\n",
      "Coefficients: [-0.36058157 -0.57901396 -0.3579421 ]\n",
      "\n",
      "--- Comparison (Absolute Difference) ---\n",
      "Intercept Diff: 0.000000\n",
      "Coef Max Diff : 0.000000\n",
      "SUCCESS: Models match closely!\n",
      "\n",
      "Mean Prediction Diff: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from glmext.glm import BinomialRegressor\n",
    "\n",
    "\n",
    "def generate_data(n_samples=1000, n_features=5, k=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for Binomial regression.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = rng.randn(n_samples, n_features)\n",
    "    # True coefficients\n",
    "    coef = rng.uniform(-1, 1, size=n_features)\n",
    "    intercept = 0.5\n",
    "    # Linear predictor\n",
    "    linear_pred = np.dot(X, coef) + intercept\n",
    "    p = 1 / (1 + np.exp(-linear_pred))\n",
    "    y = rng.binomial(n=1, p=p)\n",
    "    return X, y, coef, intercept\n",
    "\n",
    "\n",
    "def check_models():\n",
    "    # 1. Setup Data\n",
    "    n_samples = 2000\n",
    "    n_features = 3\n",
    "    k_true = 3 # Dispersion\n",
    "    X, y, true_coef, true_intercept = generate_data(n_samples, n_features, k=k_true)\n",
    "    print(f\"--- Data Generation ---\")\n",
    "    print(f\"True Dispersion (k): {k_true}\")\n",
    "    print(f\"Mean of y: {np.mean(y):.4f}, Var of y: {np.var(y):.4f}\")\n",
    "    print(f\"Ratio Var/Mean: {np.var(y)/np.mean(y):.4f} (Expect > 1)\\n\")\n",
    "\n",
    "    # 2. Statsmodels Implementation\n",
    "    # Add intercept for statsmodels\n",
    "    X_sm = sm.add_constant(X)\n",
    "    # statsmodels Binomial family\n",
    "    # loglike_method='nb2' corresponds to Var = mu + alpha * mu^2\n",
    "    sm_model = sm.GLM(\n",
    "        y,\n",
    "        X_sm,\n",
    "        family=sm.families.Binomial()\n",
    "    ).fit()\n",
    "\n",
    "    print(\"--- Statsmodels (GLM) ---\")\n",
    "    print(sm_model.summary().tables[1])\n",
    "    sm_coef = sm_model.params[1:]\n",
    "    sm_intercept = sm_model.params[0]\n",
    "    # 3. Custom Sklearn Implementation\n",
    "    # We set alpha=0 to remove regularization (matching statsmodels default)\n",
    "    # We fix k to the same value used in statsmodels\n",
    "    custom_model = BinomialRegressor(\n",
    "        alpha=0.0, # No L2 regularization\n",
    "        fit_intercept=False,\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        tol=1e-6\n",
    "    )\n",
    "    custom_model.fit(\n",
    "        X_sm, y\n",
    "    )\n",
    "    print(\"\\n--- Custom BinomialRegressor ---\")\n",
    "    print(f\"Intercept: {custom_model.coef_[0]:.4f}\")\n",
    "    print(f\"Coefficients: {custom_model.coef_[1:]}\")\n",
    "    # 4. Comparison\n",
    "    print(\"\\n--- Comparison (Absolute Difference) ---\")\n",
    "    diff_intercept = abs(sm_intercept - custom_model.coef_[0])\n",
    "    diff_coef = np.abs(sm_coef - custom_model.coef_[1:])\n",
    "    print(f\"Intercept Diff: {diff_intercept:.6f}\")\n",
    "    print(f\"Coef Max Diff : {np.max(diff_coef):.6f}\")\n",
    "    if diff_intercept < 1e-3 and np.max(diff_coef) < 1e-3:\n",
    "        print(\"SUCCESS: Models match closely!\")\n",
    "    else:\n",
    "        print(\"Models diverge. Check regularization or tolerance.\")\n",
    "\n",
    "    # 5. Prediction Check\n",
    "    y_pred_sm = sm_model.predict(X_sm)\n",
    "    y_pred_custom = custom_model.predict(X_sm)\n",
    "    print(f\"\\nMean Prediction Diff: {np.mean(np.abs(y_pred_sm - y_pred_custom)):.6f}\")\n",
    "\n",
    "\n",
    "check_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbcfd3-64e1-4182-8aa3-e1e57f1c0d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glmext",
   "language": "python",
   "name": "glmext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
